{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Michael feedback: HUGE lesson here. If you want to create well built tools/projects/libraries this is the course to start with. An extreme amount of content that could probably 6 courses alone if he went into more detail. A lot of ideas and side projects from the content in this lesson. 5/5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Python Programming Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter: Functions and Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_files(filenames):\n",
    "    # Set up the loop iteration instructions\n",
    "    for name in filenames:\n",
    "        # Use pathlib.Path to print out each file\n",
    "        print(Path(name).read_text())\n",
    "        \n",
    "def list_files(filenames):\n",
    "    # Use pathlib.Path to read the contents of each file\n",
    "    return [Path(name).read_text()\n",
    "            # Obtain each name from the list of filenames\n",
    "            for name in filenames]\n",
    "\n",
    "filenames = \"diabetes.txt\", \"boston.txt\", \"digits.txt\", \"iris.txt\", \"wine.txt\"\n",
    "print_files(filenames)\n",
    "pprint(list_files(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    return (item \n",
    "            # Obtain each list from the list of lists\n",
    "            for sublist in nested_list\n",
    "            # Obtain each element from each individual list\n",
    "            for item in sublist)\n",
    "\n",
    "number_generator = (int(substring) for string in flatten(matches)\n",
    "                    for substring in string.split() if substring.isdigit())\n",
    "pprint(dict(zip(filenames, zip(number_generator, number_generator))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic flattening! In this third coding exercise, we practiced using generator expressions instead of list comprehensions. Next, we'll continue on to learn about a new set of principles related to Python workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_words(string):\n",
    "    # Replace non-alphabetic characters with spaces\n",
    "    return \"\".join(char if char.isalpha() else \" \" for char in string).split()\n",
    "\n",
    "def filter_words(words, minimum_length=3):\n",
    "    # Remove words shorter than 3 characters\n",
    "    return [word for word in words if len(word) >= minimum_length]\n",
    "\n",
    "words = obtain_words(Path(\"diabetes.txt\").read_text().lower())\n",
    "filtered_words = filter_words(words)\n",
    "pprint(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(word_list):\n",
    "    # Count the words in the input list\n",
    "    return {word: word_list.count(word) for word in word_list}\n",
    "\n",
    "# Create the dictionary of words and word counts\n",
    "word_count_dictionary = count_words(filtered_words)\n",
    "\n",
    "(pd.DataFrame(word_count_dictionary.items())\n",
    " .sort_values(by=1, ascending=False)\n",
    " .head()\n",
    " .plot(x=0, kind=\"barh\", xticks=range(5), legend=False)\n",
    " .set_ylabel(\"\")\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful word-ranking! We now have several independent, reusable functions at our disposal. In this coding exercise, we used a dictionary comprehension to link words with their counts and then sorted the words we obtained by their frequency. In the next video, we will learn about the last principle covered in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the first parameter in the pair_plot() definition\n",
    "def pair_plot(self, vars=range(3), hue=None):\n",
    "    return pairplot(pd.DataFrame(self.data), vars=vars, hue=hue, kind=\"reg\")\n",
    "\n",
    "ScikitData.pair_plot = pair_plot\n",
    "\n",
    "# Create the diabetes instance of the ScikitData class\n",
    "diabetes = ScikitData(\"diabetes\")\n",
    "diabetes.pair_plot(vars=range(2, 6), hue=1)._legend.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we saw how classes and methods can be very useful! The choice to use seaborn for plotting is an implementation detail. We could switch to different visualization library without changing how our ScikitData class is used. Next, we'll add a class method to ScikitData that will allow us to simultaneously create more than one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the decorator for the get_generator() definition\n",
    "@classmethod\n",
    "# Add the first parameter to the get_generator() definition\n",
    "def get_generator(cls, dataset_names):\n",
    "    return map(cls, dataset_names)\n",
    "\n",
    "ScikitData.get_generator = get_generator\n",
    "dataset_generator = ScikitData.get_generator([\"diabetes\", \"iris\"])\n",
    "for dataset in dataset_generator:\n",
    "    dataset.pair_plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we have learned about abstracting away implementation details and providing interfaces to facilitate code use. For example, the details of how the ScikitData class works are hidden inside the module that contains the ScikitData class definition. We can use the ScikitData class to quickly obtain and explore scikit-learn datasets without looking at the ScikitData source code. In the next chapter, we will learn to describe how our code works with documentation and to ensure that the code works properly with testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# PART 2 - Documentation and Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFile:\n",
    "  \t# Add type hints to TextFile\"s __init__() method\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.text = Path(name).read_text()\n",
    "\n",
    "\t# Type annotate TextFile\"s get_lines() method\n",
    "    def get_lines(self) -> List[str]:\n",
    "        return self.text.split(\"\\n\")\n",
    "\n",
    "help(TextFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can be certain that the result of calling get_lines() will be a list of strings. While it is good to be as specific as possible when annotating return values, we might want to be a little more flexible with argument type hints. For example, we could design the TextFile class to allow users to pass a single filename or a list or tuple of filenames. Being smart about type hinting requires finding a balance between flexibility and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchFinder:\n",
    "  \t# Add type hints to __init__()'s strings argument\n",
    "    def __init__(self, strings: List[str]) -> None:\n",
    "        self.strings = strings\n",
    "\n",
    "\t# Type annotate get_matches()'s query argument\n",
    "    def get_matches(self, query: Optional[str] = None) -> List[str]:\n",
    "        return [s for s in self.strings if query in s] if query else self.strings\n",
    "\n",
    "help(MatchFinder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type hints help us to understand important aspects of how functions and methods work, without having to look at the source code in modules. We will continue to use type annotation throughout the course, but in the next lesson we'll shift our attention to another way we can include information in the code that we write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest / doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(word_list: List[str], query:str) -> List[str]:\n",
    "    (\"Find lines containing the query string.\\nExamples:\\n\\t\"\n",
    "     # Complete the docstring example below\n",
    "     \">>> get_matches(['a', 'list', 'of', 'words'], 's')\\n\\t\"\n",
    "     # Fill in the expected result of the function call\n",
    "     \"['list', 'words']\")\n",
    "    return [line for line in word_list if query in line]\n",
    "\n",
    "help(get_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple doctest example is a great way convey information on how an object can be used. In the next exercise, we'll look at another function definition that includes an example in its docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_words(string: str) -> List[str]:\n",
    "    (\"Get the top words in a word list.\\nExamples:\\n\\t\"\n",
    "     \">>> from this import s\\n\\t>>> from codecs import decode\\n\\t\"\n",
    "     # Use obtain_words() in the docstring example below\n",
    "     \">>> obtain_words(decode(s, encoding='rot13'))[:4]\\n\\t\"\n",
    "     # Fill in the expected result of the function call\n",
    "     \"['The', 'Zen', 'of', 'Python']\") \n",
    "    return ''.join(char if char.isalpha() else ' ' for char in string).split()\n",
    "  \n",
    "help(obtain_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doctest examples are great additions to docstrings, but later in the chapter we will learn more about testing so that we do not have have to include all of our tests in docstrings. Next, we will learn to generate reports using Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbuild(filenames: List[str]) -> nbformat.notebooknode.NotebookNode:\n",
    "    \"\"\"Create a Jupyter notebook from text files and Python scripts.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    nb.cells = [\n",
    "        # Create new code cells from files that end in .py\n",
    "        new_code_cell(Path(name).read_text()) \n",
    "        if name.endswith(\".py\")\n",
    "        # Create new markdown cells from all other files\n",
    "        else new_markdown_cell(Path(name).read_text()) \n",
    "        for name in filenames\n",
    "    ]\n",
    "    return nb\n",
    "    \n",
    "pprint(nbuild([\"intro.md\", \"plot.py\", \"discussion.md\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the output is a little bit difficult to read. Jupyter notebooks, JSON files, and NotebookNode objects are all similar to Python dictionaries. The differences between JSON files and Python dictionaries are minimal, so becoming familiar with one will help you understand the other. Next, we will save the output of nbuild() as a notebook and write a function that will allow us to convert notebooks to (almost) any format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbconv(nb_name: str, exporter: str = \"script\") -> str:\n",
    "    \"\"\"Convert a notebook into various formats using different exporters.\"\"\"\n",
    "    # Instantiate the specified exporter class\n",
    "    exp = get_exporter(exporter)()\n",
    "    # Return the converted file\"s contents string \n",
    "    return exp.from_filename(nb_name)[0]\n",
    "    \n",
    "pprint(nbconv(nb_name=\"mynotebook.ipynb\", exporter=\"html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on successfully converting a notebook to an HTML report! We could just as easily create a PDF report by passing 'pdf' to nbconv() as its exporter argument. The nbconvert library provides the means to create many different types of output documents from Jupyter notebooks. In the next video, we will learn to write pytest tests for the nbuild() function from the previous exercise. Furthermore, we will learn how to squeeze even more functionality from our docstrings by using a documentation generator called Sphinx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the decorator for the test_nbuild() function \n",
    "@pytest.mark.parametrize(\"inputs\", [\"intro.md\", \"plot.py\", \"discussion.md\"])\n",
    "# Pass the argument set to the test_nbuild() function\n",
    "def test_nbuild(inputs):\n",
    "    assert nbuild([inputs]).cells[0].source == Path(inputs).read_text()\n",
    "\n",
    "show_test_output(test_nbuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"not_exporters\", [\"htm\", \"ipython\", \"markup\"])\n",
    "# Pass the argument set to the test_nbconv() function\n",
    "def test_nbconv(not_exporters):\n",
    "     # Use pytest to confirm that a ValueError is raised\n",
    "    with pytest.raises(ValueError):\n",
    "        nbconv(nb_name=\"mynotebook.ipynb\", exporter=not_exporters)\n",
    "\n",
    "show_test_output(test_nbconv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've progressed way past doctest examples, but we've only scratched the surface of what pytest is capable of! The techniques we learned in this chapter prevent modules from turning into black boxes. While type hints, docstrings, documentation and tests do not change how modules work, users will certainly appreciate well-documented and bug-free code! In the next chapter, we will learn how Python can synergize with the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - Shell superpowers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argparse nbuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argparse_cli(func: Callable) -> None:\n",
    "    # Instantiate the parser object\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Add an argument called in_files to the parser object\n",
    "    parser.add_argument(\"in_files\", nargs=\"*\")\n",
    "    args = parser.parse_args()\n",
    "    print(func(args.in_files))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    argparse_cli(nbuild)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### docopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the section title in the docstring below\n",
    "\"\"\"Usage: docopt_cli.py [IN_FILES...]\"\"\"\n",
    "\n",
    "def docopt_cli(func: Callable) -> None:\n",
    "    # Assign the shell arguments to \"args\"\n",
    "    args = docopt(__doc__)\n",
    "    print(func(args[\"IN_FILES\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docopt_cli(nbuild)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GitPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitPython gives us building blocks that we can use to build Python scripts that make our use of version control faster, easier, and more efficient.\n",
    "\n",
    "Version controlled projects usually start with initializing or cloning repositories.\n",
    "\n",
    "After a repository is set up, the standard cycle of commands is add and commit changes.\n",
    "\n",
    "In this exercise, we will focus on the first two steps: adding changes to the index and committing them to version control history.\n",
    "\n",
    "The commit message is created by an f-string, which evaluates the code inside curly braces ({}).\n",
    "\n",
    "With GitPython, we can initialize a new repository and instantiate the Repo class in one line of code.\n",
    "\n",
    "We can then check for untracked files, add files to the index, commit changes, and list all of the newly tracked files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new repo in the current folder\n",
    "repo = git.Repo.init()\n",
    "\n",
    "# Obtain a list of untracked files\n",
    "untracked = repo.untracked_files\n",
    "\n",
    "# Add all untracked files to the index\n",
    "repo.index.add(untracked)\n",
    "\n",
    "# Commit newly added files to version control history\n",
    "repo.index.commit(f\"Added {', '.join(untracked)}\")\n",
    "print(repo.head.commit.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_files = [file.b_path\n",
    "                 # Iterate over items in the diff object\n",
    "                 for file in repo.index.diff(None)\n",
    "                 # Include only modified files\n",
    "                 .iter_change_type(\"M\")]\n",
    "\n",
    "repo.index.add(changed_files)\n",
    "repo.index.commit(f\"Modified {', '.join(changed_files)}\")\n",
    "for number, commit in enumerate(repo.iter_commits()):\n",
    "    print(number, commit.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding and commiting changes are the most common tasks in git workflows. Diffs let us inspect the tracked and untracked changes since the latest commit. Version control is an important aspect of every workflow, so even though you've finished the last GitPython exercises, we will certainly return to git later in the course. In the next video, we will learn how to set up and track the libraries that our code needs to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment managers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for libraries, run commands in the shell through subprocess, code below prints out library youre looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# Create an virtual environment\n",
    "venv.create(\".venv\")\n",
    "\n",
    "# Run pip list and obtain a CompletedProcess instance\n",
    "cp = subprocess.run([\".venv/bin/python\", \"-m\", \"pip\", \"list\"], stdout=-1)\n",
    "\n",
    "for line in cp.stdout.decode().split(\"\\n\"):\n",
    "    if \"pandas\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run(\n",
    "    # Install project dependencies\n",
    "    [\".venv/bin/python\", \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "    stdout=-1\n",
    ").stdout.decode())\n",
    "\n",
    "print(run(\n",
    "    # Show information on the aardvark package\n",
    "    [\".venv/bin/python\", \"-m\", \"pip\", \"show\", \"aardvark\"], stdout=-1\n",
    ").stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistence and packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle and unpickle a dataframe (and plot) - Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.c_[(diabetes.data, diabetes.target)],\n",
    "    columns=\"age sex bmi map tc ldl hdl tch ltg glu target\".split()\n",
    "    # Pickle the diabetes dataframe with zip compression\n",
    "    ).to_pickle(\"diabetes.pkl.zip\")\n",
    "                  \n",
    "# Unpickle the diabetes dataframe\n",
    "df = pd.read_pickle(\"diabetes.pkl.zip\")\n",
    "df.plot.scatter(x=\"ltg\", y=\"target\", c=\"age\", colormap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle and unpickle a dataframe (and plot) - Joblib - Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and pickle a linear model\n",
    "joblib.dump(LinearRegression().fit(x_train, y_train), \"linear.pkl\")\n",
    "\n",
    "# Unpickle the linear model\n",
    "linear = joblib.load(\"linear.pkl\")\n",
    "predictions = linear.predict(x_test)\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "min_max = [y_test.min(), y_test.max()]\n",
    "plt.plot(min_max, min_max, \"--\", lw=3)\n",
    "plt.xlabel(\"Measured\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4 - Projects, pipelines, and parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cookie cutter for template | Sphinx for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up JSON cookiecutter template file\n",
    "\n",
    "json_path.write_text(json.dumps({\n",
    "    \"project\": \"Creating Robust Python Workflows\",\n",
    "  \t# Convert the project name into snake_case\n",
    "    \"package\": \"{{ cookiecutter.project.lower().replace(' ', '_') }}\",\n",
    "    # Fill in the default license value\n",
    "    \"license\": [\"MIT\", \"BSD\", \"GPL3\"]\n",
    "}))\n",
    "\n",
    "pprint(json.loads(json_path.read_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json and pathlib modules to set up template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain keys from the local template's cookiecutter.json\n",
    "keys = [*json.load(json_path.open())]\n",
    "vals = \"Your name here\", \"My Amazing Python Project\"\n",
    "\n",
    "# Create a cookiecutter project without prompting for input\n",
    "main.cookiecutter(template_root.as_posix(), no_input=True,\n",
    "                  extra_context=dict(zip(keys, vals)))\n",
    "\n",
    "for path in pathlib.Path.cwd().glob(\"**\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we used the pathlib and json modules from the standard library to learn more about a local Cookiecutter template and then used the template to set up a project with a custom name. Projects templates are a powerful automation tool that can be combined with Make and Sphinx to save tremendous amounts of time and standardize project structure and contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipapp - Create a zipped project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipapp.create_archive(\n",
    "    # Zip up a project called \"myproject\"\n",
    "    \"myproject\",                    \n",
    "    interpreter=\"/usr/bin/env python\",\n",
    "    # Generate a __main__.py file\n",
    "    main=\"mypackage.mymodule:print_name_and_file\")\n",
    "\n",
    "print(subprocess.run([\".venv/bin/python\", \"myproject.pyz\"],\n",
    "                     stdout=-1).stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Scikit datasets only!\")\n",
    "    # Set the default for the dataset argument\n",
    "    parser.add_argument(\"dataset\", nargs=\"?\", default=\"diabetes\")\n",
    "    parser.add_argument(\"model\", nargs=\"?\", default=\"linear_model.Ridge\")\n",
    "    args = parser.parse_args()\n",
    "    # Create a dictionary of the shell arguments\n",
    "    kwargs = dict(dataset=args.dataset, model=args.model)\n",
    "    return (classify(**kwargs) if args.dataset in (\"digits\", \"iris\", \"wine\")\n",
    "            else regress(**kwargs) if args.dataset in (\"boston\", \"diabetes\")\n",
    "            else print(f\"{args.dataset} is not a supported dataset!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we learned how to set up a command-line interface for an executable project. Even if you do not plan to run projects in a shell very often, being able to pass command-line arguments to a zipped project is pretty cool! This is a great example of enjoying the convenience of a single file without sacrificing modularity! In the next video, we will learn how to pass parameters to Jupyter notebooks using the papermill library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the notebook to find the default parameter names\n",
    "pprint(nbformat.read(\"sklearn.ipynb\", as_version=4).cells[0].source)\n",
    "keys = [\"dataset_name\", \"model_type\", \"model_name\", \"hyperparameters\"]\n",
    "vals = [\"diabetes\", \"ensemble\", \"RandomForestRegressor\",\n",
    "        dict(max_depth=3, n_estimators=100, random_state=0)]\n",
    "parameter_dictionary = dict(zip(keys, vals))\n",
    "\n",
    "# Execute the notebook with custom parameters\n",
    "pprint(pm.execute_notebook(\n",
    "    \"sklearn.ipynb\", \"rf_diabetes.ipynb\", \n",
    "    kernel_name=\"python3\", parameters=parameter_dictionary\n",
    "\t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this exercise, we accessed a notebook cell and executed a notebook programmatically using papermill. In the next exercise, we will use scrapbook to programmatically access notebook data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapbook as sb\n",
    "\n",
    "# Assign the scrapbook notebook object to nb\n",
    "nb = sb.read_notebook(\"rf_diabetes.ipynb\")\n",
    "\n",
    "# Create a dataframe of scraps (recorded values)\n",
    "scrap_df = nb.scrap_dataframe\n",
    "print(scrap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parallel computing w/ dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read in a csv file using a dask.dataframe method\n",
    "df = dd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "df[\"bin_age\"] = (df.age > 0).astype(int)\n",
    "\n",
    "# Compute the columns means in the two age groups\n",
    "print(df.groupby(\"bin_age\").mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the penultimate exercise of this course, we used created and persisted a Dask dataframe. We also showed how we can end a method chain with compute() to obtain a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### joblib - In the last exercise of this course, we will use the grid search technique to find the optimal hyperparameters for an elastic net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Dask client with 4 threads and 1 worker\n",
    "Client(processes=False, threads_per_worker=4, n_workers=1)\n",
    "\n",
    "# Run grid search using joblib and a Dask backend\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    engrid.fit(x_train, y_train)\n",
    "\n",
    "plot_enet(*enet_path(x_test, y_test, eps=5e-5, fit_intercept=False,\n",
    "                    l1_ratio=engrid.best_params_[\"l1_ratio\"])[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
